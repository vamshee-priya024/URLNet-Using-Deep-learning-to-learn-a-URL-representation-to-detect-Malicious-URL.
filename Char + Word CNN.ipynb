{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "in_csv = 'data.csv'\n",
    "number_lines = 420444\n",
    "rowsize = 210222\n",
    "for i in range(1,number_lines,rowsize):\n",
    "    df = pd.read_csv(in_csv,\n",
    "          header=None,\n",
    "          nrows = rowsize,#number of rows to read at each loop\n",
    "          skiprows = i)#skip rows that have been read\n",
    "    out_csv = 'word_char' + str(i) + '.csv'\n",
    "    df.to_csv(out_csv,\n",
    "          index=False,\n",
    "          header=False,\n",
    "          mode='a',#append data to csv file\n",
    "          chunksize=rowsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data_source = 'word_char1.csv'\n",
    "test_data_source = 'word_char210223.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_data_source, header=None)\n",
    "test_df = pd.read_csv(test_data_source, header=None)\n",
    "\n",
    "max_len=train_df[0].map(lambda x: len(x)).max()\n",
    "\n",
    "# concatenate column 1 and column 2 as one text\n",
    "for df in [train_df, test_df]:\n",
    "    df[0] = df[0] + df[1]\n",
    "    df = df.drop([1], axis=1)\n",
    "\n",
    "train_df = train_df.sample(frac = 1)\n",
    "test_df = test_df.sample(frac = 1)\n",
    "\n",
    "# convert string to lower case \n",
    "train_texts = train_df[1].values \n",
    "train_texts = [s.lower() for s in train_texts] \n",
    "\n",
    "test_texts = test_df[1].values \n",
    "test_texts = [s.lower() for s in test_texts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "\n",
    "# construct a new vocabulary \n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz0123456789 ,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "    \n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy() \n",
    "# Add 'UNK' to the vocabulary \n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = test_df.sample(frac = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3284740</th>\n",
       "      <td>petindustrynews.blogspot.com/2011/11/pet-produ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814075</th>\n",
       "      <td>angelfire.com/wa2/hwysofwastate/sr509.htmlgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605245</th>\n",
       "      <td>cfandfibroliving.com/wp-content/plugins/s/anme...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099008</th>\n",
       "      <td>international.uiowa.edu/services/language/alln...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488148</th>\n",
       "      <td>ecreatify.com/lof/Reviladate/mail.htm?cmd=LOB=...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517376</th>\n",
       "      <td>trempepiette.fortunecity.com/idxc.htmgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2315353</th>\n",
       "      <td>chananpk.org/wp-includes/cd/index.phpbad</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505867</th>\n",
       "      <td>en.wikipedia.org/wiki/Thomas_G._Rosenmeyergood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185755</th>\n",
       "      <td>politicalgraveyard.com/bio/morris.htmlgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124258</th>\n",
       "      <td>en-pi.facebook.com/sandra.reed1good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582545</th>\n",
       "      <td>lbiexposed.blogspot.com/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476819</th>\n",
       "      <td>bollywooddvdstore.com/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953671</th>\n",
       "      <td>local.yahoo.com/info-15861551-metropolitan-hou...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239162</th>\n",
       "      <td>en.wikipedia.org/wiki/University-preparatory_s...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038947</th>\n",
       "      <td>starkstreetphotography.blogspot.com/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052538</th>\n",
       "      <td>kinetic.more.net/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122374</th>\n",
       "      <td>music.yahoo.com/paul-felton/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354329</th>\n",
       "      <td>tg.bigcartel.com/artist/sabertooth-zombiegood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972415</th>\n",
       "      <td>en.wikipedia.org/wiki/John_Farrellgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4334310</th>\n",
       "      <td>penguins.nhl.com/club/page.htm?id=63928good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0     1\n",
       "3284740  petindustrynews.blogspot.com/2011/11/pet-produ...  good\n",
       "814075      angelfire.com/wa2/hwysofwastate/sr509.htmlgood  good\n",
       "3605245  cfandfibroliving.com/wp-content/plugins/s/anme...   bad\n",
       "4099008  international.uiowa.edu/services/language/alln...  good\n",
       "1488148  ecreatify.com/lof/Reviladate/mail.htm?cmd=LOB=...   bad\n",
       "3517376          trempepiette.fortunecity.com/idxc.htmgood  good\n",
       "2315353           chananpk.org/wp-includes/cd/index.phpbad   bad\n",
       "505867      en.wikipedia.org/wiki/Thomas_G._Rosenmeyergood  good\n",
       "1185755         politicalgraveyard.com/bio/morris.htmlgood  good\n",
       "1124258                en-pi.facebook.com/sandra.reed1good  good\n",
       "1582545                       lbiexposed.blogspot.com/good  good\n",
       "476819                          bollywooddvdstore.com/good  good\n",
       "953671   local.yahoo.com/info-15861551-metropolitan-hou...  good\n",
       "3239162  en.wikipedia.org/wiki/University-preparatory_s...  good\n",
       "2038947           starkstreetphotography.blogspot.com/good  good\n",
       "3052538                              kinetic.more.net/good  good\n",
       "122374                    music.yahoo.com/paul-felton/good  good\n",
       "4354329      tg.bigcartel.com/artist/sabertooth-zombiegood  good\n",
       "1972415             en.wikipedia.org/wiki/John_Farrellgood  good\n",
       "4334310        penguins.nhl.com/club/page.htm?id=63928good  good"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>938684</th>\n",
       "      <td>newspaperabstracts.com/link.php?action=detail&amp;...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51411</th>\n",
       "      <td>imdb.com/name/nm0000265/biogood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4215206</th>\n",
       "      <td>drdavidberg.net/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971351</th>\n",
       "      <td>fanbase.com/Kansas-State-Wildcats-Mens-Basketb...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033482</th>\n",
       "      <td>gettingaway.com/directory/mexico/States/Chihua...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373143</th>\n",
       "      <td>yelp.ca/biz/restaurant-chez-cora-dejeuner-mont...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980677</th>\n",
       "      <td>gadnet.com/movies.htmgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377536</th>\n",
       "      <td>youtube.com/watch?v=MXJSUbyR6kogood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426873</th>\n",
       "      <td>realestateinsanfernandovalley.net/Home_Service...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379986</th>\n",
       "      <td>yelp.ca/biz/patinoire-atrium-le-1000-de-la-gau...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690973</th>\n",
       "      <td>youtube.com/watch?v=Tvx0m5blnH4good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488908</th>\n",
       "      <td>goldsilver.com/new/gonzalo-lira-a-beginner-s-g...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900412</th>\n",
       "      <td>youtube.com/watch?v=OAEbcUYtKV0good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960465</th>\n",
       "      <td>linkedin.com/company/rio-tinto-alcangood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280131</th>\n",
       "      <td>mahalo.com/susan-pinker/good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290289</th>\n",
       "      <td>theage.com.au/entertainment/movies/sienna-mill...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241746</th>\n",
       "      <td>filmsite.org/independentfilms.htmlgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043415</th>\n",
       "      <td>hsc.on.ca/discover-hsc/builders-patrons.htmlgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291523</th>\n",
       "      <td>mp3raid.com/music/marcus_foster.htmlgood</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976250</th>\n",
       "      <td>flickr.com/photos/ontariogenomicsinstitute/520...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0     1\n",
       "938684   newspaperabstracts.com/link.php?action=detail&...  good\n",
       "51411                      imdb.com/name/nm0000265/biogood  good\n",
       "4215206                               drdavidberg.net/good  good\n",
       "2971351  fanbase.com/Kansas-State-Wildcats-Mens-Basketb...  good\n",
       "4033482  gettingaway.com/directory/mexico/States/Chihua...  good\n",
       "373143   yelp.ca/biz/restaurant-chez-cora-dejeuner-mont...  good\n",
       "2980677                          gadnet.com/movies.htmgood  good\n",
       "377536                 youtube.com/watch?v=MXJSUbyR6kogood  good\n",
       "2426873  realestateinsanfernandovalley.net/Home_Service...  good\n",
       "4379986  yelp.ca/biz/patinoire-atrium-le-1000-de-la-gau...  good\n",
       "2690973                youtube.com/watch?v=Tvx0m5blnH4good  good\n",
       "2488908  goldsilver.com/new/gonzalo-lira-a-beginner-s-g...  good\n",
       "2900412                youtube.com/watch?v=OAEbcUYtKV0good  good\n",
       "1960465           linkedin.com/company/rio-tinto-alcangood  good\n",
       "4280131                       mahalo.com/susan-pinker/good  good\n",
       "3290289  theage.com.au/entertainment/movies/sienna-mill...  good\n",
       "241746              filmsite.org/independentfilms.htmlgood  good\n",
       "4043415   hsc.on.ca/discover-hsc/builders-patrons.htmlgood  good\n",
       "4291523           mp3raid.com/music/marcus_foster.htmlgood  good\n",
       "2976250  flickr.com/photos/ontariogenomicsinstitute/520...  good"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_class_list=list(train_df[1].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "train_classes = pd.Categorical(pd.factorize(train_class_list)[0])\n",
    "print (set(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Categories (2, int64): [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_classes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_classes = to_categorical(train_classes)\n",
    "print(train_classes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_class_list=list(test_df[1].values)\n",
    "test_classes = pd.Categorical(pd.factorize(test_class_list)[0])\n",
    "print (set(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "test_classes = to_categorical(test_classes)\n",
    "print(test_classes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "test_texts = tk.texts_to_sequences(test_texts)\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=182, padding='post')\n",
    "test_data = pad_sequences(test_texts, maxlen=182, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='int32')\n",
    "test_data = np.array(test_data, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 15 15 ...  0  0  0]\n",
      " [ 7 15 15 ...  0  0  0]\n",
      " [ 2  1  4 ...  0  0  0]\n",
      " ...\n",
      " [ 7 15 15 ...  0  0  0]\n",
      " [ 7 15 15 ...  0  0  0]\n",
      " [ 7 15 15 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, ' ': 37, ',': 38, ';': 39, '.': 40, '!': 41, '?': 42, ':': 43, \"'\": 44, '\"': 45, '/': 46, '\\\\': 47, '|': 48, '_': 49, '@': 50, '#': 51, '$': 52, '%': 53, '^': 54, '&': 55, '*': 56, '~': 57, '`': 58, '+': 59, '-': 60, '=': 61, '<': 62, '>': 63, '(': 64, ')': 65, '[': 66, ']': 67, '{': 68, '}': 69, 'UNK': 70}\n"
     ]
    }
   ],
   "source": [
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = [] #(71, 70)\n",
    "embedding_weights.append(np.zeros(vocab_size)) # first row is pad\n",
    "\n",
    "for char, i in tk.word_index.items(): # from index 1 to 70\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 70)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_weights.shape) # first row all 0 for PAD, 69 char, last row for UNK\n",
    "embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(\"positive.csv\", \"r\", encoding='latin-1').readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(\"negative.csv\", \"r\", encoding='latin-1').readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    random.shuffle(x_text)\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    random.shuffle(y)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    np.random.shuffle(x)\n",
    "    np.random.shuffle(y)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding,Conv1D,Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading data')\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776287\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    geeky_file = open('vocabulary_word_char.txt', 'wt')\n",
    "    geeky_file.write(str(vocabulary))\n",
    "    geeky_file.close()\n",
    "\n",
    "except:\n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written successfully\n"
     ]
    }
   ],
   "source": [
    "with open('vocabulary_inv_word_char.txt', 'w+') as f:\n",
    "    for items in vocabulary_inv:\n",
    "        f.write('%s\\n' %items)\n",
    "    print(\"File written successfully\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336372\n",
      "336372\n",
      "84094\n",
      "84094\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88684     43 764595 ... 489858 489858 489858]\n",
      " [336811     43 758233 ... 489858 489858 489858]\n",
      " [462883     43 569880 ... 489858 489858 489858]\n",
      " ...\n",
      " [250852     43 649065 ... 489858 489858 489858]\n",
      " [206316     43 598569 ... 489858 489858 489858]\n",
      " [157830     43 546736 ... 489858 489858 489858]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1] \n",
    "vocabulary_size = len(vocabulary_inv) \n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding1 = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(word_inputs)\n",
    "reshape1 = Reshape((sequence_length,embedding_dim))(embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776287\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 182, 256), dtype=tf.float32, name=None), name='embedding/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding'\")\n"
     ]
    }
   ],
   "source": [
    "print(embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 182, 256), dtype=tf.float32, name=None), name='reshape/Reshape:0', description=\"created by layer 'reshape'\")\n"
     ]
    }
   ],
   "source": [
    "print(reshape1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding,Conv1D,MaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0]), padding='valid', kernel_initializer='normal', activation='relu')(reshape1)\n",
    "conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1]), padding='valid', kernel_initializer='normal', activation='relu')(reshape1)\n",
    "#conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 179, 512), dtype=tf.float32, name=None), name='conv1d_1/Relu:0', description=\"created by layer 'conv1d_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(conv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool_0 = MaxPool1D(pool_size=(sequence_length - filter_sizes[0] + 1), strides=(1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool1D(pool_size=(sequence_length - filter_sizes[1] + 1), strides=(1), padding='valid')(conv_1)\n",
    "#maxpool_2 = MaxPool1D(pool_size=(sequence_length - filter_sizes[2] + 1), strides=(1,1), padding='valid')(conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 512), dtype=tf.float32, name=None), name='max_pooling1d_1/Squeeze:0', description=\"created by layer 'max_pooling1d_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(maxpool_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor_1 = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "flatten1 = Flatten()(concatenated_tensor_1)\n",
    "#dropout = Dropout(drop)(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n"
     ]
    }
   ],
   "source": [
    "print(flatten1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding2 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length)(char_inputs)\n",
    "reshape2 = Reshape((sequence_length,embedding_dim))(embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 182, 256), dtype=tf.float32, name=None), name='embedding_1/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 182, 256), dtype=tf.float32, name=None), name='reshape_1/Reshape:0', description=\"created by layer 'reshape_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(reshape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[0]), padding='valid', kernel_initializer='normal', activation='relu')(reshape2)\n",
    "conv_3 = Conv1D(num_filters, kernel_size=(filter_sizes[1]), padding='valid', kernel_initializer='normal', activation='relu')(reshape2)\n",
    "#conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 179, 512), dtype=tf.float32, name=None), name='conv1d_3/Relu:0', description=\"created by layer 'conv1d_3'\")\n"
     ]
    }
   ],
   "source": [
    "print(conv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool_2 = MaxPool1D(pool_size=(sequence_length - filter_sizes[0] + 1), strides=(1), padding='valid')(conv_2)\n",
    "maxpool_3 = MaxPool1D(pool_size=(sequence_length - filter_sizes[1] + 1), strides=(1), padding='valid')(conv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1, 512), dtype=tf.float32, name=None), name='max_pooling1d_3/Squeeze:0', description=\"created by layer 'max_pooling1d_3'\")\n"
     ]
    }
   ],
   "source": [
    "print(maxpool_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor_2 = Concatenate(axis=1)([maxpool_2, maxpool_3])\n",
    "flatten2 = Flatten()(concatenated_tensor_2)\n",
    "#dropout = Dropout(drop)(flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1024), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n"
     ]
    }
   ],
   "source": [
    "print(flatten2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_tensor_3 = Concatenate(axis=1)([flatten1, flatten2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='concatenate_2/concat:0', description=\"created by layer 'concatenate_2'\")\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_tensor_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Dropout(drop)(concatenated_tensor_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2048), dtype=tf.float32, name=None), name='dropout/Identity:0', description=\"created by layer 'dropout'\")\n"
     ]
    }
   ],
   "source": [
    "print(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Dense(units=512, activation='relu')(dropout)\n",
    "dense2 = Dense(units=128, activation='relu')(dense1)\n",
    "dense3 = Dense(units=1, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense_2/Softmax:0', description=\"created by layer 'dense_2'\")\n"
     ]
    }
   ],
   "source": [
    "print(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a model \n",
    "model = Model(inputs=[word_inputs,char_inputs], outputs=[dense3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(inputs=[word_inputs,char_inputs], outputs=[dense3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=[]\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i][1]==0):\n",
    "        Y_train.append(0)\n",
    "    else:\n",
    "        Y_train.append(1)\n",
    "Y_train = np.array(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=[]\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i][1]==0):\n",
    "        Y_test.append(0)\n",
    "    else:\n",
    "        Y_test.append(1)\n",
    "Y_test = np.array(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "167/167 [==============================] - 649s 4s/step - loss: 0.1619 - accuracy: 0.9846 - val_loss: 0.0740 - val_accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[tf.keras.metrics.Accuracy(name='accuracy')])\n",
    "print(\"Traning Model...\")\n",
    "history=model.fit((X_train[:5000],train_data[:5000]),Y_train[:5000], batch_size=batch_size, epochs=epochs, verbose=1, validation_data=([X_test[:3000],test_data[:3000]],Y_test[:3000]))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n",
      "334/334 [==============================] - 1157s 3s/step - loss: 0.0816 - binary_accuracy: 0.9846 - val_loss: 0.0926 - val_binary_accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model1.compile(optimizer=adam, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "history=model1.fit((X_train[:10000],train_data[:10000]),Y_train[:10000], batch_size=batch_size, epochs=epochs, verbose=1, validation_data=([X_test[:5000],test_data[:5000]],Y_test[:5000]))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_word-cnn\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('char_word-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: charWord-cnns\\assets\n"
     ]
    }
   ],
   "source": [
    "model1.save('charWord-cnns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 182)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 182)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 182, 256)     198729472   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 182, 256)     17920       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 182, 256)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 182, 256)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 180, 512)     393728      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 179, 512)     524800      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 180, 512)     393728      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 179, 512)     524800      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 512)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 512)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 512)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 512)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2, 512)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 512)       0           max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1024)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2048)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          1049088     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          65664       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 201,699,329\n",
      "Trainable params: 201,699,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X_test[3000:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[113436     43 528819 ... 489858 489858 489858]\n",
      " [344533     43 769674 ... 489858 489858 489858]\n",
      " [420652     43 521174 ... 489858 489858 489858]\n",
      " ...\n",
      " [ 79184     43 748849 ... 489858 489858 489858]\n",
      " [241422     43 639221 ... 489858 489858 489858]\n",
      " [ 71272     43 639418 ... 489858 489858 489858]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_data[3000:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict([X_test[3000:5000],test_data[3000:5000]]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_hat = y_hat. astype(int)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(len(y_hat)):\n",
    "    if(y_hat[i]==0):\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84094\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_test))\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(\n",
    "    Y_test[3000:5000], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                con.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     con.flatten()/np.sum(con)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(labels,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(con, annot=labels, fmt='', cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      1.00      0.99      1960\n",
      "           0       0.00      0.00      0.00        40\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.49      0.50      0.49      2000\n",
      "weighted avg       0.96      0.98      0.97      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhanu\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "matrix = classification_report(Y_test[3000:5000],y_hat,labels=[1,0])\n",
    "print('Classification report : \\n',matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('char_word-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sample = 'chananpk.org/wp-includes/cd/index.php'\n",
    "X_sample=X_sample.lower()\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "X_sample = tk.texts_to_sequences(X_sample)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=182)\n",
    "#y_sample = model.predict(X_sample).flatten().tolist()\n",
    "#X_sample=X_sample[:-1]\n",
    "#print('Prediction: ',y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_sample = 'chananpk.org/wp-includes/cd/index.php'\n",
    "X2_sample=clean_str(X2_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X2_sample)\n",
    "X2_sample = tk.texts_to_sequences(X2_sample)\n",
    "X2_sample = pad_sequences(X2_sample, padding='post', maxlen=182)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = model1.predict([X_sample,X2_sample]).flatten().tolist()\n",
    "#print(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ans=np.argmax(y_sample)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sample = 'politicalgraveyard.com/bio/morris.html'\n",
    "X_sample=clean_str(X_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X_sample)\n",
    "X_sample = tk.texts_to_sequences(X_sample)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=182)\n",
    "#y_sample = model.predict(X_sample).flatten().tolist()\n",
    "\n",
    "X2_sample = 'politicalgraveyard.com/bio/morris.html'\n",
    "X2_sample=clean_str(X2_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X2_sample)\n",
    "X2_sample = tk.texts_to_sequences(X2_sample)\n",
    "X2_sample = pad_sequences(X2_sample, padding='post', maxlen=182)\n",
    "\n",
    "y_sample = model1.predict([X_sample,X2_sample]).flatten().tolist()\n",
    "ans=np.argmax(y_sample)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sample = 'morris.html'\n",
    "X_sample=clean_str(X_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X_sample)\n",
    "X_sample = tk.texts_to_sequences(X_sample)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=182)\n",
    "#y_sample = model.predict(X_sample).flatten().tolist()\n",
    "\n",
    "X2_sample = 'morris.html'\n",
    "X2_sample=clean_str(X2_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X2_sample)\n",
    "X2_sample = tk.texts_to_sequences(X2_sample)\n",
    "X2_sample = pad_sequences(X2_sample, padding='post', maxlen=182)\n",
    "\n",
    "y_sample = model.predict([X_sample,X2_sample]).flatten().tolist()\n",
    "ans=np.argmax(y_sample)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
