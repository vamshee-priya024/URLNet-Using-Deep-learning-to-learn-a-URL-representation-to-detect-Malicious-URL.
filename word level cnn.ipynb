{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diaryofagameaddict.com</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>espdesign.com.au</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iamagameaddict.com</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalantzis.net</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slightlyoffcenter.net</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      url label\n",
       "0  diaryofagameaddict.com   bad\n",
       "1        espdesign.com.au   bad\n",
       "2      iamagameaddict.com   bad\n",
       "3           kalantzis.net   bad\n",
       "4   slightlyoffcenter.net   bad"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      url label\n",
      "42767                                          01453.com/  good\n",
      "42768               015fb31.netsolhost.com/bosstweed.html  good\n",
      "42769             02bee66.netsolhost.com/lincolnhomepage/  good\n",
      "42770   02ec0a3.netsolhost.com/getperson.php?personID=...  good\n",
      "42771                                         032255.com/  good\n",
      "...                                                   ...   ...\n",
      "387583  youthleaguesusa.com/potomacsoccer/2011/Tournam...  good\n",
      "387584                                     zip-codes.com/  good\n",
      "387585                    owens.edu/news-releases/?p=2052  good\n",
      "387586      1.safesecureweb.com/egale/index.asp?item=1173  good\n",
      "387587               yurika.otakuthon.com/reg/main.pl/en/  good\n",
      "\n",
      "[344821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_positive = df[df['label'] == 'good']\n",
    "  \n",
    "print(df_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           url label\n",
      "0       diaryofagameaddict.com   bad\n",
      "1             espdesign.com.au   bad\n",
      "2           iamagameaddict.com   bad\n",
      "3                kalantzis.net   bad\n",
      "4        slightlyoffcenter.net   bad\n",
      "...                        ...   ...\n",
      "420459         23.227.196.215/   bad\n",
      "420460      apple-checker.org/   bad\n",
      "420461       apple-iclods.org/   bad\n",
      "420462      apple-uptoday.org/   bad\n",
      "420463       apple-search.info   bad\n",
      "\n",
      "[75643 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_negative = df[df['label'] == 'bad']\n",
    "  \n",
    "print(df_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive.to_csv('positive.csv')\n",
    "df_negative.to_csv('negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(\"positive.csv\", \"r\", encoding='latin-1').readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(\"negative.csv\", \"r\", encoding='latin-1').readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    sentences, labels = load_data_and_labels()\n",
    "    sentences_padded = pad_sentences(sentences)\n",
    "    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "    x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "    return [x, y, vocabulary, vocabulary_inv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "[[    43 752806     43 ... 489858 489858 489858]\n",
      " [403329     43   1371 ... 489858 489858 489858]\n",
      " [403331     43   1411 ... 489858 489858 489858]\n",
      " ...\n",
      " [402235     43 503104 ... 489858 489858 489858]\n",
      " [402236     43 503104 ... 489858 489858 489858]\n",
      " [402237     43 503104 ... 489858 489858 489858]] [[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading data')\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776287\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    geeky_file = open('vocabulary.txt', 'wt')\n",
    "    geeky_file.write(str(vocabulary))\n",
    "    geeky_file.close()\n",
    "\n",
    "except:\n",
    "    print(\"Unable to write to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File written successfully\n"
     ]
    }
   ],
   "source": [
    "with open('vocabulary_inv.txt', 'w+') as f:\n",
    "    for items in vocabulary_inv:\n",
    "        f.write('%s\\n' %items)\n",
    "    print(\"File written successfully\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[224452     43 619046 ... 489858 489858 489858]\n",
      " [ 71985     43 736306 ... 489858 489858 489858]\n",
      " [302041     43 718341 ... 489858 489858 489858]\n",
      " ...\n",
      " [103000     43 497321 ... 489858 489858 489858]\n",
      " [121144     43 507341 ... 489858 489858 489858]\n",
      " [ 90948     43 766226 ... 489858 489858 489858]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = x.shape[1] \n",
    "vocabulary_size = len(vocabulary_inv) \n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Traning Model...\n",
      "561/561 [==============================] - 2065s 4s/step - loss: 0.2415 - accuracy: 0.9066 - val_loss: 0.0021 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model \n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "#checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "history=model.fit(X_train[:16818], y_train[:16818], batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test[:4204], y_test[:4204]))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word-cnn\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('word-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstructed_model = keras.models.load_model(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.3802046775817871, 0.6197953820228577, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.36733871698379517, 0.6326612830162048, 0.3802046775817871, 0.6197953820228577, 0.36733871698379517, 0.6326612830162048, 0.3673386871814728, 0.6326612830162048, 0.3673386871814728, 0.6326612830162048]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sample = 'pos-kupang.com/'\n",
    "X_sample=clean_str(X_sample)\n",
    "tk = Tokenizer(num_words=True, oov_token='UNK')\n",
    "tk.fit_on_texts(X_sample)\n",
    "X_sample = tk.texts_to_sequences(X_sample)\n",
    "X_sample = pad_sequences(X_sample, padding='post', maxlen=182)\n",
    "y_sample = model.predict(X_sample).flatten().tolist()\n",
    "\n",
    "print('Prediction: ',y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ans=np.argmax(y_sample)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
